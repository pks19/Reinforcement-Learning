{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reinforcement_learning_Taxi_v3_OpenAI.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2rJfjLmjOImE","colab_type":"code","colab":{}},"source":["## Solving the same problem using DeepQNetwork\n","import gym\n","import random\n","import numpy as np\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","from keras.layers import Embedding\n","from keras.layers import Reshape\n","from keras.layers.advanced_activations import LeakyReLU, PReLU\n","from keras import regularizers\n","#from scores.score_logger import ScoreLogger"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EaBdvODlywsY","colab_type":"code","colab":{}},"source":["class DQNSolver:\n","    def __init__(self, observation_space, action_space):\n","        self.exploration_rate = EXPLORATION_MAX\n","        self.action_space = action_space\n","        self.memory = deque(maxlen=MEMORY_SIZE)\n","        self.vocab_size = VOCAB_SIZE\n","        self.embed_dim = EMBED_DIM\n","\n","        # Define a feedforward NN\n","        self.model = Sequential()\n","        self.model.add(Embedding(self.vocab_size, self.embed_dim, input_shape=(observation_space,)))        \n","        self.model.add(Reshape((self.embed_dim,)))\n","        self.model.add(Dense(30, activation=\"relu\", kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l2(0.01))) \n","        self.model.add(Dense(self.action_space, activation=\"linear\")) \n","        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))\n","\n","    # Create a memory sequence\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    # Given a state predict action\n","    def act(self, state):\n","        if np.random.rand() < self.exploration_rate:\n","            return random.choice(range(self.action_space))            \n","        q_values = self.model.predict(state)\n","        return np.argmax(q_values[0])\n","\n","    def experience_replay(self):\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","        batch = random.sample(self.memory, BATCH_SIZE)\n","        for state, action, reward, state_next, terminal in batch:\n","            q_update = reward\n","            if not terminal:\n","                q_update = reward + GAMMA * np.amax(self.model.predict(state_next)[0])\n","            q_values = self.model.predict(state)\n","            q_values[0][action] = q_update\n","            self.model.fit(state, q_values, verbose=0)\n","        \n","        self.exploration_rate *= EXPLORATION_DECAY\n","        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zQq6M497OKDN","colab_type":"code","colab":{}},"source":["# OpenAI Environment \n","ENV_NAME = \"Taxi-v3\"\n","\n","# Hyperparameters\n","GAMMA = 0.7\n","LEARNING_RATE = 0.1\n","MEMORY_SIZE = 50000\n","BATCH_SIZE = 128\n","EXPLORATION_MAX = 1\n","EXPLORATION_MIN = 0.5\n","EXPLORATION_DECAY = 0.995\n","EPISODES = 5000\n","VOCAB_SIZE = 500 # env.observation_space gives the vocabulary size for embedding layer\n","EMBED_DIM = 16\n","\n","# Instantiate env object\n","env = gym.make(ENV_NAME).env\n","observation_space = 1 \n","action_space = env.action_space.n -1\n","\n","# Create DQN solver instance\n","dqn_solver = DQNSolver(observation_space, action_space)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LFrzdIW3kOei","colab_type":"code","colab":{}},"source":["# Train for a number of episodes\n","run = 0\n","for i in range(EPISODES):\n","    run += 1\n","    state = np.reshape(env.reset(), [1, observation_space])\n","    step = 0\n","    terminal = False\n","    penalties = 0\n","    while True:\n","        step += 1\n","        action = dqn_solver.act(state)\n","        state_next, reward, terminal, info = env.step(action)\n","        if(action ==4): terminal = True\n","        state_next = np.reshape(state_next, [1, observation_space])\n","        dqn_solver.remember(state, action, reward, state_next, terminal)\n","        state = state_next\n","        if terminal:\n","            print(\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))\n","            break\n","    dqn_solver.experience_replay()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UGzj6V1OQKa","colab_type":"code","colab":{}},"source":[" \"\"\"Evaluate agent's performance after Deep Q-learning\"\"\"\n","\n","total_epochs, total_penalties, total_reward_all_episode = 0, 0, 0\n","episodes = 1\n","for e in range(episodes):\n","    state = env.reset()\n","    env.render()\n","    state = np.reshape(state, [1, observation_space])\n","    epochs, penalties, reward, total_reward = 0, 0, 0, 0    \n","    done = False\n","    while not done:\n","        action = dqn_solver.act(state)\n","        state, reward, done, info = env.step(action)\n","        env.render()\n","        if action == 4: done =True\n","        state = np.reshape(state, [1, observation_space])\n","        total_reward += reward \n","        if reward == -10:\n","            penalties += 1\n","        epochs += 1      \n","    total_penalties += penalties\n","    total_epochs += epochs\n","    total_reward_all_episode +=total_reward\n","    print(f\"Total reward for Episode {e}:  {total_reward}\") \n","    print(f\"Penalty for Episode {e}:  {penalties}\") \n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties}\")\n","print(f\"Average reward per episode: {total_reward_all_episode / episodes}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUmG01DEpwDY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}