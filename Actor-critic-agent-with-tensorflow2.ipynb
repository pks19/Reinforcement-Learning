{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Actor-critic-agent-with-tensorflow2.ipynb","provenance":[{"file_id":"12QvW7VZSzoaF-Org-u-N6aiTdBN5ohNA","timestamp":1554160365326}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rPMkxVFBg85m","colab_type":"text"},"source":["# Deep Reinforcement Learning with TensorFlow 2.0"]},{"cell_type":"markdown","metadata":{"id":"0IFjMkr_g4ZK","colab_type":"text"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"08aIvFN7hNhl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598605023960,"user_tz":-120,"elapsed":1877,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}},"outputId":"4640e335-d8ea-4341-e0ed-32777975c03f"},"source":["!pip install tf-nightly-2.0-preview"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for tf-nightly-2.0-preview\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QipBW2W7g4ZL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598605025735,"user_tz":-120,"elapsed":3645,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}}},"source":["import gym\n","import logging\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.keras.layers as kl\n","import tensorflow.keras.losses as kls\n","import tensorflow.keras.optimizers as ko"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyx5ah4Xg4ZO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598605025737,"user_tz":-120,"elapsed":3639,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}}},"source":["import matplotlib\n","import matplotlib.pyplot as plt\n","%matplotlib inline  "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"kmjplMPwg4ZR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1598605025738,"user_tz":-120,"elapsed":3545,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}},"outputId":"c65c3cdf-b1d0-4267-d160-35922efa1507"},"source":["print(\"TensorFlow Ver: \", tf.__version__)\n","print(\"Eager Execution:\", tf.executing_eagerly())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["TensorFlow Ver:  2.3.0\n","Eager Execution: True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lr62U1IPg4ZU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598605031030,"user_tz":-120,"elapsed":8800,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}},"outputId":"b45049fc-1a28-47a0-cb1b-6f552b801ca3"},"source":["# eager by default!\n","print(\"1 + 2 + 3 + 4 + 5 =\", tf.reduce_sum([1, 2, 3, 4, 5]))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1 + 2 + 3 + 4 + 5 = tf.Tensor(15, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1kO-Mnllg4ZX","colab_type":"text"},"source":["## Advantage Actor-Critic with TensorFlow 2.0"]},{"cell_type":"markdown","metadata":{"id":"jFGPAkQNg4ZX","colab_type":"text"},"source":["### Policy & Value Model Class"]},{"cell_type":"code","metadata":{"id":"wIQ8YHmxg4ZZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598605031031,"user_tz":-120,"elapsed":8794,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}}},"source":["class ProbabilityDistribution(tf.keras.Model):\n","    def call(self, logits):\n","        # sample a random categorical action from given logits\n","        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n","\n","class Model(tf.keras.Model):\n","    def __init__(self, num_actions):\n","        super().__init__('mlp_policy')\n","        # no tf.get_variable(), just simple Keras API\n","        self.hidden1 = kl.Dense(128, activation='relu')\n","        self.hidden2 = kl.Dense(128, activation='relu')\n","        self.value = kl.Dense(1, name='value')\n","        # logits are unnormalized log probabilities\n","        self.logits = kl.Dense(num_actions, name='policy_logits')\n","        self.dist = ProbabilityDistribution()\n","\n","    def call(self, inputs):\n","        # inputs is a numpy array, convert to Tensor\n","        x = tf.convert_to_tensor(inputs)\n","        # separate hidden layers from the same input tensor\n","        hidden_logs = self.hidden1(x)\n","        hidden_vals = self.hidden2(x)\n","        return self.logits(hidden_logs), self.value(hidden_vals)\n","\n","    def action_value(self, obs):\n","        # executes call() under the hood\n","        logits, value = self.predict(obs)\n","        action = self.dist.predict(logits)\n","        # a simpler option, will become clear later why we don't use it\n","        # action = tf.random.categorical(logits, 1)\n","        return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_g8dFW1rg4Zb","colab_type":"text"},"source":["### Advantage Actor-Critic Agent Class"]},{"cell_type":"code","metadata":{"id":"M-pKLzu2g4Zb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598605031032,"user_tz":-120,"elapsed":8791,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}}},"source":["class A2CAgent:\n","    def __init__(self, model):\n","        # hyperparameters for loss terms, gamma is the discount coefficient\n","        self.params = {\n","            'gamma': 0.99,\n","            'value': 0.5,\n","            'entropy': 0.0001\n","        }\n","        self.model = model\n","        self.model.compile(\n","            optimizer=ko.RMSprop(lr=0.0007),\n","            # define separate losses for policy logits and value estimate\n","            loss=[self._logits_loss, self._value_loss]\n","        )\n","    \n","    def train(self, env, batch_sz=32, updates=1000):\n","        # storage helpers for a single batch of data\n","        actions = np.empty((batch_sz,), dtype=np.int32)\n","        rewards, dones, values = np.empty((3, batch_sz))\n","        observations = np.empty((batch_sz,) + env.observation_space.shape)\n","        # training loop: collect samples, send to optimizer, repeat updates times\n","        ep_rews = [0.0]\n","        next_obs = env.reset()\n","        for update in range(updates):\n","            for step in range(batch_sz):\n","                observations[step] = next_obs.copy()\n","                actions[step], values[step] = self.model.action_value(next_obs[None, :])\n","                next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n","\n","                ep_rews[-1] += rewards[step]\n","                if dones[step]:\n","                    ep_rews.append(0.0)\n","                    next_obs = env.reset()\n","                    logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rews)-1, ep_rews[-2]))\n","\n","            _, next_value = self.model.action_value(next_obs[None, :])\n","            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n","            # a trick to input actions and advantages through same API\n","            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n","            # performs a full training step on the collected batch\n","            # note: no need to mess around with gradients, Keras API handles it\n","            losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n","            logging.debug(\"[%d/%d] Losses: %s\" % (update+1, updates, losses))\n","        return ep_rews\n","\n","    def test(self, env, render=False):\n","        obs, done, ep_reward = env.reset(), False, 0\n","        while not done:\n","            action, _ = self.model.action_value(obs[None, :])\n","            obs, reward, done, _ = env.step(action)\n","            ep_reward += reward\n","            if render:\n","                env.render()\n","        return ep_reward\n","\n","    def _returns_advantages(self, rewards, dones, values, next_value):\n","        # next_value is the bootstrap value estimate of a future state (the critic)\n","        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n","        # returns are calculated as discounted sum of future rewards\n","        for t in reversed(range(rewards.shape[0])):\n","            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n","        returns = returns[:-1]\n","        # advantages are returns - baseline, value estimates in our case\n","        advantages = returns - values\n","        return returns, advantages\n","    \n","    def _value_loss(self, returns, value):\n","        # value loss is typically MSE between value estimates and returns\n","        return self.params['value']*kls.mean_squared_error(returns, value)\n","\n","    def _logits_loss(self, acts_and_advs, logits):\n","        # a trick to input actions and advantages through same API\n","        actions, advantages = tf.split(acts_and_advs, 2, axis=-1)\n","        # sparse categorical CE loss obj that supports sample_weight arg on call()\n","        # from_logits argument ensures transformation into normalized probabilities\n","        weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n","        # policy loss is defined by policy gradients, weighted by advantages\n","        # note: we only calculate the loss on the actions we've actually taken\n","        actions = tf.cast(actions, tf.int32)\n","        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n","        # entropy loss can be calculated via CE over itself\n","        entropy_loss = kls.categorical_crossentropy(logits, logits, from_logits=True)\n","        # here signs are flipped because optimizer minimizes\n","        return policy_loss - self.params['entropy']*entropy_loss"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"yf1I04Ttg4Ze","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598605033860,"user_tz":-120,"elapsed":11588,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}},"outputId":"41b16db4-52f7-4af2-e182-edc1bf608e0d"},"source":["env = gym.make('CartPole-v0')\n","model = Model(num_actions=env.action_space.n)\n","model.action_value(env.reset()[None, :])"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(0), array([8.099154e-05], dtype=float32))"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"n8LFt3keg4Zh","colab_type":"text"},"source":["## Training A2C Agent & Results"]},{"cell_type":"code","metadata":{"id":"1mxzrB1Ng4Zi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598605033863,"user_tz":-120,"elapsed":11586,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}}},"source":["env = gym.make('CartPole-v0')\n","model = Model(num_actions=env.action_space.n)\n","agent = A2CAgent(model)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KR7v0mBig4Zl","colab_type":"text"},"source":["### Testing with Random Weights"]},{"cell_type":"code","metadata":{"id":"k3JFA1pDg4Zl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1598605036253,"user_tz":-120,"elapsed":13942,"user":{"displayName":"Prashant Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF5PluG3oPC-1oWM-5m1Opn39M9a4wAWRI40NLGg=s64","userId":"13837900537794814748"}},"outputId":"7b5d1ef2-b928-4950-85d1-106e47e9933e"},"source":["rewards_sum = agent.test(env)\n","print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Total Episode Reward: 12 out of 200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"25FRn0aRg4Zo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"dfcdd1e2-ed68-483a-a0f3-3f38b3df70ec"},"source":["# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n","logging.getLogger().setLevel(logging.INFO)\n","\n","rewards_history = agent.train(env)\n","print(\"Finished training.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:root:Episode: 001, Reward: 013\n","INFO:root:Episode: 002, Reward: 018\n","INFO:root:Episode: 003, Reward: 016\n","INFO:root:Episode: 004, Reward: 048\n","INFO:root:Episode: 005, Reward: 018\n","INFO:root:Episode: 006, Reward: 009\n","INFO:root:Episode: 007, Reward: 038\n","INFO:root:Episode: 008, Reward: 010\n","INFO:root:Episode: 009, Reward: 066\n","INFO:root:Episode: 010, Reward: 017\n","INFO:root:Episode: 011, Reward: 025\n","INFO:root:Episode: 012, Reward: 014\n","INFO:root:Episode: 013, Reward: 021\n","INFO:root:Episode: 014, Reward: 012\n","INFO:root:Episode: 015, Reward: 017\n","INFO:root:Episode: 016, Reward: 018\n","INFO:root:Episode: 017, Reward: 048\n","INFO:root:Episode: 018, Reward: 062\n","INFO:root:Episode: 019, Reward: 022\n","INFO:root:Episode: 020, Reward: 028\n","INFO:root:Episode: 021, Reward: 029\n","INFO:root:Episode: 022, Reward: 018\n","INFO:root:Episode: 023, Reward: 014\n","INFO:root:Episode: 024, Reward: 013\n","INFO:root:Episode: 025, Reward: 030\n","INFO:root:Episode: 026, Reward: 014\n","INFO:root:Episode: 027, Reward: 026\n","INFO:root:Episode: 028, Reward: 083\n","INFO:root:Episode: 029, Reward: 012\n","INFO:root:Episode: 030, Reward: 013\n","INFO:root:Episode: 031, Reward: 023\n","INFO:root:Episode: 032, Reward: 036\n","INFO:root:Episode: 033, Reward: 036\n","INFO:root:Episode: 034, Reward: 014\n","INFO:root:Episode: 035, Reward: 030\n","INFO:root:Episode: 036, Reward: 046\n","INFO:root:Episode: 037, Reward: 016\n","INFO:root:Episode: 038, Reward: 019\n","INFO:root:Episode: 039, Reward: 011\n","INFO:root:Episode: 040, Reward: 047\n","INFO:root:Episode: 041, Reward: 012\n","INFO:root:Episode: 042, Reward: 019\n","INFO:root:Episode: 043, Reward: 018\n","INFO:root:Episode: 044, Reward: 033\n","INFO:root:Episode: 045, Reward: 018\n","INFO:root:Episode: 046, Reward: 021\n","INFO:root:Episode: 047, Reward: 023\n","INFO:root:Episode: 048, Reward: 015\n","INFO:root:Episode: 049, Reward: 023\n","INFO:root:Episode: 050, Reward: 028\n","INFO:root:Episode: 051, Reward: 046\n","INFO:root:Episode: 052, Reward: 035\n","INFO:root:Episode: 053, Reward: 015\n","INFO:root:Episode: 054, Reward: 017\n","INFO:root:Episode: 055, Reward: 012\n","INFO:root:Episode: 056, Reward: 020\n","INFO:root:Episode: 057, Reward: 056\n","INFO:root:Episode: 058, Reward: 011\n","INFO:root:Episode: 059, Reward: 064\n","INFO:root:Episode: 060, Reward: 060\n","INFO:root:Episode: 061, Reward: 022\n","INFO:root:Episode: 062, Reward: 035\n","INFO:root:Episode: 063, Reward: 059\n","INFO:root:Episode: 064, Reward: 013\n","INFO:root:Episode: 065, Reward: 014\n","INFO:root:Episode: 066, Reward: 064\n","INFO:root:Episode: 067, Reward: 042\n","INFO:root:Episode: 068, Reward: 061\n","INFO:root:Episode: 069, Reward: 035\n","INFO:root:Episode: 070, Reward: 014\n","INFO:root:Episode: 071, Reward: 023\n","INFO:root:Episode: 072, Reward: 024\n","INFO:root:Episode: 073, Reward: 050\n","INFO:root:Episode: 074, Reward: 052\n","INFO:root:Episode: 075, Reward: 045\n","INFO:root:Episode: 076, Reward: 023\n","INFO:root:Episode: 077, Reward: 021\n","INFO:root:Episode: 078, Reward: 011\n","INFO:root:Episode: 079, Reward: 032\n","INFO:root:Episode: 080, Reward: 043\n","INFO:root:Episode: 081, Reward: 038\n","INFO:root:Episode: 082, Reward: 021\n","INFO:root:Episode: 083, Reward: 019\n","INFO:root:Episode: 084, Reward: 035\n","INFO:root:Episode: 085, Reward: 016\n","INFO:root:Episode: 086, Reward: 047\n","INFO:root:Episode: 087, Reward: 013\n","INFO:root:Episode: 088, Reward: 013\n","INFO:root:Episode: 089, Reward: 054\n","INFO:root:Episode: 090, Reward: 032\n","INFO:root:Episode: 091, Reward: 032\n","INFO:root:Episode: 092, Reward: 017\n","INFO:root:Episode: 093, Reward: 034\n","INFO:root:Episode: 094, Reward: 022\n","INFO:root:Episode: 095, Reward: 025\n","INFO:root:Episode: 096, Reward: 015\n","INFO:root:Episode: 097, Reward: 022\n","INFO:root:Episode: 098, Reward: 023\n","INFO:root:Episode: 099, Reward: 022\n","INFO:root:Episode: 100, Reward: 017\n","INFO:root:Episode: 101, Reward: 026\n","INFO:root:Episode: 102, Reward: 046\n","INFO:root:Episode: 103, Reward: 034\n","INFO:root:Episode: 104, Reward: 060\n","INFO:root:Episode: 105, Reward: 018\n","INFO:root:Episode: 106, Reward: 045\n","INFO:root:Episode: 107, Reward: 079\n","INFO:root:Episode: 108, Reward: 033\n","INFO:root:Episode: 109, Reward: 070\n","INFO:root:Episode: 110, Reward: 017\n","INFO:root:Episode: 111, Reward: 056\n","INFO:root:Episode: 112, Reward: 050\n","INFO:root:Episode: 113, Reward: 024\n","INFO:root:Episode: 114, Reward: 090\n","INFO:root:Episode: 115, Reward: 042\n","INFO:root:Episode: 116, Reward: 032\n","INFO:root:Episode: 117, Reward: 057\n","INFO:root:Episode: 118, Reward: 029\n","INFO:root:Episode: 119, Reward: 031\n","INFO:root:Episode: 120, Reward: 051\n","INFO:root:Episode: 121, Reward: 048\n","INFO:root:Episode: 122, Reward: 121\n","INFO:root:Episode: 123, Reward: 050\n","INFO:root:Episode: 124, Reward: 077\n","INFO:root:Episode: 125, Reward: 019\n","INFO:root:Episode: 126, Reward: 031\n","INFO:root:Episode: 127, Reward: 045\n","INFO:root:Episode: 128, Reward: 041\n","INFO:root:Episode: 129, Reward: 066\n","INFO:root:Episode: 130, Reward: 020\n","INFO:root:Episode: 131, Reward: 056\n","INFO:root:Episode: 132, Reward: 037\n","INFO:root:Episode: 133, Reward: 069\n","INFO:root:Episode: 134, Reward: 012\n","INFO:root:Episode: 135, Reward: 027\n","INFO:root:Episode: 136, Reward: 118\n","INFO:root:Episode: 137, Reward: 096\n","INFO:root:Episode: 138, Reward: 028\n","INFO:root:Episode: 139, Reward: 028\n","INFO:root:Episode: 140, Reward: 061\n","INFO:root:Episode: 141, Reward: 050\n","INFO:root:Episode: 142, Reward: 025\n","INFO:root:Episode: 143, Reward: 030\n","INFO:root:Episode: 144, Reward: 027\n","INFO:root:Episode: 145, Reward: 023\n","INFO:root:Episode: 146, Reward: 098\n","INFO:root:Episode: 147, Reward: 042\n","INFO:root:Episode: 148, Reward: 053\n","INFO:root:Episode: 149, Reward: 042\n","INFO:root:Episode: 150, Reward: 036\n","INFO:root:Episode: 151, Reward: 062\n","INFO:root:Episode: 152, Reward: 015\n","INFO:root:Episode: 153, Reward: 028\n","INFO:root:Episode: 154, Reward: 102\n","INFO:root:Episode: 155, Reward: 051\n","INFO:root:Episode: 156, Reward: 044\n","INFO:root:Episode: 157, Reward: 031\n","INFO:root:Episode: 158, Reward: 016\n","INFO:root:Episode: 159, Reward: 118\n","INFO:root:Episode: 160, Reward: 084\n","INFO:root:Episode: 161, Reward: 041\n","INFO:root:Episode: 162, Reward: 038\n","INFO:root:Episode: 163, Reward: 015\n","INFO:root:Episode: 164, Reward: 072\n","INFO:root:Episode: 165, Reward: 056\n","INFO:root:Episode: 166, Reward: 024\n","INFO:root:Episode: 167, Reward: 036\n","INFO:root:Episode: 168, Reward: 039\n","INFO:root:Episode: 169, Reward: 046\n","INFO:root:Episode: 170, Reward: 039\n","INFO:root:Episode: 171, Reward: 052\n","INFO:root:Episode: 172, Reward: 040\n","INFO:root:Episode: 173, Reward: 041\n","INFO:root:Episode: 174, Reward: 062\n","INFO:root:Episode: 175, Reward: 057\n","INFO:root:Episode: 176, Reward: 037\n","INFO:root:Episode: 177, Reward: 020\n","INFO:root:Episode: 178, Reward: 109\n","INFO:root:Episode: 179, Reward: 041\n","INFO:root:Episode: 180, Reward: 037\n","INFO:root:Episode: 181, Reward: 049\n","INFO:root:Episode: 182, Reward: 034\n","INFO:root:Episode: 183, Reward: 021\n","INFO:root:Episode: 184, Reward: 025\n","INFO:root:Episode: 185, Reward: 038\n","INFO:root:Episode: 186, Reward: 030\n","INFO:root:Episode: 187, Reward: 055\n","INFO:root:Episode: 188, Reward: 021\n","INFO:root:Episode: 189, Reward: 045\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"UJ7Ss140g4Zs","colab_type":"text"},"source":["### Testing with Trained Model"]},{"cell_type":"code","metadata":{"id":"lbM_P6CMg4Zt","colab_type":"code","colab":{}},"source":["print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RFDN0Smg4Zv","colab_type":"text"},"source":["### Training Rewards History"]},{"cell_type":"code","metadata":{"id":"ou6PDK8ig4Zx","colab_type":"code","colab":{}},"source":["plt.style.use('seaborn')\n","plt.plot(np.arange(0, len(rewards_history), 25), rewards_history[::25])\n","plt.xlabel('Episode')\n","plt.ylabel('Total Reward')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zrtyfS8Sg4Z0","colab_type":"text"},"source":["## Static Computational Graph"]},{"cell_type":"code","metadata":{"id":"5WA6tTTrg4Z1","colab_type":"code","colab":{}},"source":["with tf.Graph().as_default():\n","    print(\"Eager Execution:\", tf.executing_eagerly()) # False\n","\n","    model = Model(num_actions=env.action_space.n)\n","    agent = A2CAgent(model)\n","\n","    rewards_history = agent.train(env)\n","    print(\"Finished training, testing...\")\n","    print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0k55LpUOg4Z4","colab_type":"text"},"source":["## Benchmarks"]},{"cell_type":"code","metadata":{"id":"aR5J2WCZg4Z5","colab_type":"code","colab":{}},"source":["# Note: comparing wall time isn't exactly fair due to specifics of how things are executed on multi-core CPU"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fl51m1PXg4Z8","colab_type":"code","colab":{}},"source":["env = gym.make('CartPole-v0')\n","obs = np.repeat(env.reset()[None, :], 100000, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Y6LxDvBg4Z9","colab_type":"text"},"source":["### Eager Benchmark"]},{"cell_type":"code","metadata":{"id":"8CpTEYnxg4Z_","colab_type":"code","colab":{}},"source":["%%time\n","\n","model = Model(env.action_space.n)\n","model.run_eagerly = True\n","\n","print(\"Eager Execution:  \", tf.executing_eagerly())\n","print(\"Eager Keras Model:\", model.run_eagerly)\n","\n","_ = model(obs)\n","# _ = model.predict(obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-9FMyk-g4aB","colab_type":"text"},"source":["### Static Benchmark"]},{"cell_type":"code","metadata":{"id":"t3RqVlslg4aB","colab_type":"code","colab":{}},"source":["%%time\n","\n","with tf.Graph().as_default():\n","    model = Model(env.action_space.n)\n","\n","    print(\"Eager Execution:  \", tf.executing_eagerly())\n","    print(\"Eager Keras Model:\", model.run_eagerly)\n","\n","    _ = model.predict(obs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vXKB5vcsg4aG","colab_type":"text"},"source":["### Default Benchmark"]},{"cell_type":"code","metadata":{"id":"3aqkh1yTg4aH","colab_type":"code","colab":{}},"source":["%%time\n","\n","model = Model(env.action_space.n)\n","\n","print(\"Eager Execution:  \", tf.executing_eagerly())\n","print(\"Eager Keras Model:\", model.run_eagerly)\n","\n","_ = model.predict(obs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7U8tTD8u0uW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}